---
layout: single
title: "Experience"
permalink: /Experience/
date: 2023-8-12
categories: pages
---

<div style="text-align: justify;">
  <p style="line-height: 1.5; font-size: 17px;">
      <strong><br> UNIVERSITY OF ARIZONA, DEPARTMENT OF PEDIATRICS 
      </strong><br>                          
            <strong>Graduate Research Assistant&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong>
                  <i>Feb 2023 - Present</i>
   </p>
     <br>
       <p style="line-height: 1.5;font-size: 15px;">
        
1. <strong>Thorough Data Extraction and Standardization in REDCap:</strong><br><br>
   Extract data from various sources and transform them into standardized formats and build analytical project databases in REDCap. This involves a meticulous approach to ensure that data is accurately captured and formatted for further analysis.
<br><br>
2. <strong>Optimization of Database Programs for Seamless Querying:</strong><br><br>
   Optimize database programs to facilitate efficient data querying processes; thereby enhancing the performance of database systems, allowing users to retrieve the required information rapidly and without encountering performance constraints.
<br><br>
3. <strong>Precise Data Cleaning and Linkage Protocols with Comprehensive Documentation:</strong> <br><br>
    Execute data cleaning and data linkage protocols with a high degree of precision. This involves identifying and rectifying inaccuracies, inconsistencies, and missing values within datasets. Also, maintain detailed work logs that document every step of the data refinement process, ensuring the reproducibility and bolstering the credibility of subsequent analyses.
<br><br>
4. <strong>Creation of Data Pipelines from MariaDB to PostgreSQL using Python:</strong><br><br>
   Leverage Python scripting, to design and implement data pipelines that transfer data from MariaDB to PostgreSQL, transforming and mapping the data to adhere to OMOP (Observational Medical Outcomes Partnership) table structures. Enforcing security protocols for data transfers, ensuring system integrity, and maintaining 100% compliance with data protection regulations.
<br><br>
5. <strong>Efficient Integration of MTurk Tasks and REDCap Surveys with Data Analysis:</strong>
<br><br>
   Manage the integration of MTurk tasks and REDCap surveys into a cohesive workflow. Conduct thorough data analysis using R programming after survey completion, aiding in faster comprehension and informed decision-making.
<br><br>
6. <strong>Statistical Analysis of Patient Data to Uncover Care Patterns and Demographics:
</strong><br>
   Conduct statistical tests and regression analysis on patient data to reveal insights like care trends, demographics, and factors impacting engagement. This approach identifies trends, correlations, and potential causal links, informing healthcare strategies and improving patient care.
 <br>
  </p>       
     <br>
      <p style="line-height: 1.5;font-size: 18px;">
         <strong>TATA CONSULTANCY SERVICES </strong><br>                                           
                  <strong> Systems Engineer&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</strong>
                  <i style="line-height: 1.5;font-size: 15px;">Mar 2018 - July 2022</i>
       </p>
     <br>
       <p style="line-height: 1.5;font-size: 15px;">

<strong>ETL experience working with Informatica PowerCenter 9.x</strong>:<br>
<br>
1. Utilized Informatica Power Exchange and PowerCenter 9.x for data extraction, transformation, and loading from a range of sources—including mainframes, flat files, Teradata databases, and EDW systems—have been expertly executed.<br>
<br>
2. Developed intricate ETL mappings, worklets, and reusable transformations has been a focus. These encompass filters, expressions, joiners, aggregators, and more, ensuring precise and purposeful data transformation.<br>
<br>
3. Estahblished and maintained dynamic data pipelines to support business intelligence, reporting, and analytics needs. This approach guaranteed smooth data integration and processing.<br>
<br>
<strong>UNIX and shell scripting for validation testing</strong>:<br>
<br>
1. Developed UNIX shell scripts tailored for specific validation testing requirements. Leveraged these scripts to validate data integrity and accuracy within complex datasets. Additionally, customized PLSQL scripts to further enhance the validation process, ensuring comprehensive and reliable testing outcomes.<br>
<br>
<strong>Performance tuning and optimization</strong>:<br>
<br>
1. Identified and addressed performance bottlenecks within long-running CI/CD jobs. Employed advanced optimization techniques at various levels—source, target, mapping, and session—ensuring streamlined processes and efficient data flows.<br><br>
2. Leveraged the power of cloud computing, particularly in AWS, to significantly optimize ETL operations. Achieved remarkable results with a 50% reduction in processing time, alongside heightened scalability, demonstrating a clear commitment to efficient and agile data processing.<br><br>
3. Demonstrated proficiency in utilizing ETL tools to streamline data integration workflows. This approach led to a reduction in errors by 28%, enhancement of data quality, and 2.5% cost savings on infrastructure, showcasing an ability to balance effectiveness and efficiency in data management.<br>
<br>
<strong>Database object migration</strong><br>
<br>
1. Managed the migration of crucial database objects across a range of environments, including Development, Testing, UAT, and Production. Ensured a smooth transition of these objects, maintaining data integrity and consistency throughout the lifecycle. This approach facilitated multi-stage deployment, enhancing robustness and reliability.<br>
<br>
<strong>Documentation and Project Tracking</strong>:<br>
<br>
1. Engaged in collaboration with onshore and offshore data stewards as well as application development leads to ensure smooth project tracking through JIRA. This joint endeavor enabled effective communication, task handling, and progress oversight, resulting in coordinated project implementation.<br><br>
2. Oversaw ETL code repositories through continuous integration techniques. Carried out comprehensive code evaluations to maintain rigorous code excellence and compliance with guidelines, guaranteeing the dependability and manageability of the ETL procedures.<br><br>
3. Demonstrated adeptness in articulating findings and insights to stakeholders in a clear and impactful manner. This effective communication approach facilitated informed decision-making and fostered a shared understanding of project progress and outcomes.<br><br>

<strong>Leadership</strong>:<br>
<br>
1. Offered valuable technical guidance and oversaw review processes in a dynamic and fast-paced setting, demonstrating the ability to work effectively with minimal supervision. Ensured high-quality outcomes and maintained governance standards despite the challenging environment.<br>
<br>
2. Mentored a team of 12 junior developers, sharing best practices for ETL job development using Informatica. Additionally, adeptly communicated project status updates to foster transparency and cohesion within the team, contributing to efficient workflows.<br>
<br>
3. Provided expert guidance and comprehensive training to junior ETL developers. Covered essential ETL development methodologies, data modeling best practices, and fundamental data integration concepts. This mentoring effort empowered the team with essential skills and knowledge for optimized data workflows.<br>
</p>



